{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30cd934f-6e4d-4677-ad8a-3eead4596223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77fdeee7-c1ea-4d00-8142-5299636b5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f65cf79b-9537-442f-a660-76fd6fc6d06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(4096, 4096),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4096, 4096)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbc5c4dd-f326-49f8-a88c-219248cc88e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e4f909a-dde1-440d-83e7-aed21e4c5452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.0371742770075798\n",
      "loss:  0.036889269948005676\n",
      "loss:  0.036593616008758545\n",
      "loss:  0.036329906433820724\n",
      "loss:  0.03667307645082474\n",
      "loss:  0.036307454109191895\n",
      "loss:  0.03571825474500656\n",
      "loss:  0.036094337701797485\n",
      "loss:  0.036077916622161865\n",
      "loss:  0.03585782274603844\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    x = torch.randn(32, 4096, device=device)\n",
    "\n",
    "    with autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        y = model(x)\n",
    "        loss = y.pow(2).mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print(\"loss: \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dff8df22-699a-4a74-a986-f5b97b24e5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 410720 KiB | 672928 KiB |  12141 MiB |  11740 MiB |\n",
      "|       from large pool | 409856 KiB | 672000 KiB |  11920 MiB |  11520 MiB |\n",
      "|       from small pool |    864 KiB |   3937 KiB |    221 MiB |    220 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 410720 KiB | 672928 KiB |  12141 MiB |  11740 MiB |\n",
      "|       from large pool | 409856 KiB | 672000 KiB |  11920 MiB |  11520 MiB |\n",
      "|       from small pool |    864 KiB |   3937 KiB |    221 MiB |    220 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 410720 KiB | 672928 KiB |  12141 MiB |  11740 MiB |\n",
      "|       from large pool | 409856 KiB | 672000 KiB |  11920 MiB |  11520 MiB |\n",
      "|       from small pool |    864 KiB |   3936 KiB |    221 MiB |    220 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 747520 KiB | 747520 KiB | 747520 KiB |      0 B   |\n",
      "|       from large pool | 741376 KiB | 741376 KiB | 741376 KiB |      0 B   |\n",
      "|       from small pool |   6144 KiB |   6144 KiB |   6144 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   5023 KiB |  15967 KiB | 318955 KiB | 313932 KiB |\n",
      "|       from large pool |   3840 KiB |  12160 KiB |  12160 KiB |   8320 KiB |\n",
      "|       from small pool |   1183 KiB |   4255 KiB | 306795 KiB | 305612 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      17    |      28    |    1096    |    1079    |\n",
      "|       from large pool |       8    |      12    |     248    |     240    |\n",
      "|       from small pool |       9    |      18    |     848    |     839    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      17    |      28    |    1096    |    1079    |\n",
      "|       from large pool |       8    |      12    |     248    |     240    |\n",
      "|       from small pool |       9    |      18    |     848    |     839    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      16    |      16    |      16    |       0    |\n",
      "|       from large pool |      13    |      13    |      13    |       0    |\n",
      "|       from small pool |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       3    |       7    |     348    |     345    |\n",
      "|       from large pool |       1    |       1    |       1    |       0    |\n",
      "|       from small pool |       2    |       6    |     347    |     345    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb96ce0d-a683-40c3-a607-4b8482b60eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 401.09 MB\n",
      "Reserved:  730.00 MB\n"
     ]
    }
   ],
   "source": [
    "stats = torch.cuda.memory_stats()\n",
    "\n",
    "def to_mb(x):\n",
    "    return x / 1024 / 1024\n",
    "\n",
    "print(f\"Allocated: {to_mb(stats['allocated_bytes.all.current']):.2f} MB\")\n",
    "print(f\"Reserved:  {to_mb(stats['reserved_bytes.all.current']):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49baad74-c5ec-4f7b-9806-0facdc92a443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.2)\n",
      "Collecting pip\n",
      "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.2\n",
      "    Uninstalling pip-25.2:\n",
      "      Successfully uninstalled pip-25.2\n",
      "Successfully installed pip-25.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "257ad988-0bc7-4192-bff0-8215f41b1f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn==2.8.3+cu12torch2.8cxx11abitrue\n",
      "  Downloading https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp312-cp312-linux_x86_64.whl (256.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/256.0 MB\u001b[0m \u001b[31m433.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (2.8.0+cu128)\n",
      "Collecting einops (from flash-attn==2.8.3+cu12torch2.8cxx11abitrue)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn==2.8.3+cu12torch2.8cxx11abitrue) (3.0.3)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops, flash-attn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [flash-attn]2\u001b[0m [flash-attn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed einops-0.8.1 flash-attn-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --no-cache-dir \\\n",
    "  \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.8cxx11abiTRUE-cp312-cp312-linux_x86_64.whl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18689c02-328f-4027-9451-c0120f2b94e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: flash-attn in /usr/local/lib/python3.12/dist-packages (2.8.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu128)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from flash-attn) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install flash-attn --no-build-isolation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bd51cf0-680a-46ac-8541-bee59a0764e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn import flash_attn_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a42e6671-62c3-4d55-8184-6a1b0afabd43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4096, 32, 128])\n"
     ]
    }
   ],
   "source": [
    "q = torch.randn(1, 4096, 32, 128, device=\"cuda\", dtype=torch.bfloat16)\n",
    "k = torch.randn_like(q)\n",
    "v = torch.randn_like(q)\n",
    "\n",
    "out = flash_attn_func(q, k, v, causal=True)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
